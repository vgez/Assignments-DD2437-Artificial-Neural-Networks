# DD2437 Artificial Neural Networks and Deep Architectures - Group Assignments

Complete assignment coursework for DD2437 at KTH Royal Institute of Technology. The labs focused on the implementation and experimentation of a variety of network architectures such as the Multilayer Perceptron, Self-organising Maps, Hopfield Networks, Restricted Boltzmann Machines and Deep Belief Nets. Most networks were implemented from scratch in Python 3 and NumPy. A large amount of experiments were performed across the assignments. Classification and regression tasks both supervised and unsupervised.

## Team Members

<ul>
    <li>
        <strong>Maximilian Auer</strong> - <i style="text-decoration: none;">maue@kth.se</i>
    </li>
    <li>
        <strong>Lukas Fr√∂sslund</strong> - <i style="text-decoration: none;">lukasfro@kth.se</i>
    </li>
    <li>
        <strong>Valdemar Gezelius</strong> - <i style="text-decoration: none;">vgez@kth.se</i>
    </li>  
</ul>

## Technologies

-   [Python 3](https://www.python.org/)
-   [NumPy](https://numpy.org/)

## Assignment Details

### Assignment 1

This assignment focused on feed-forward architectures and the backpropagation algorithm. Implementing backpropagation from scratch gave a deep understanding of the algorithm and its inner workings. Both classification and regression tasks were performed.

### Assignment 2

In this assignment, the focus was on RBF-networks, Self-organising Maps and Competitive Learning. It was interesting to be introduced to a completely new learning paradigm, and using non-labeled real-world data to perform unsupervised tasks.

### Assignment 3

Here the focus was on Hopfield Networks. A rather outdated network architecture, but nonetheless interesting to study and implement. Many aspects of the Hopfield Network were experimented with, such as the Energy function, sparse patterns and distortion resistance.

### Assignment 4

In the final assignment, focus was on deep neural networks and more specifically Restricted Boltzmann Machines (RBMs) and Deep Belief Nets (DBNs). Image recognition tasks were performed on both architectures. Greedy layer-wise pretraining was explored on the DBNs, consisting of stacked RBM architectures. The generation task was also explored using Gibbs sampling.

More details available in the <a href="https://github.com/Frosslund/Assignments-DD2437-Artificial-Neural-Networks/tree/main/Assignment%20Reports">assignment reports</a>
